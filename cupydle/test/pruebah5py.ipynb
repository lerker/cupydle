{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "a = np.random.random(size=(100,20))\n",
    "h5f = h5py.File('data.h5', 'w')\n",
    "h5f.create_dataset('dataset_1', data=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File('data.h5','r')\n",
    "b = h5f['dataset_1'][:]\n",
    "h5f.close()\n",
    "\n",
    "np.allclose(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "h5f = h5py.File('data2.h5', 'w')\n",
    "a = np.random.random(size=(100000,200))\n",
    "b = [1,2,3,4]\n",
    "d1=h5f.create_dataset('dataset_1', data=a,compression=\"gzip\", compression_opts=9)\n",
    "d2=h5f.create_dataset('dataset_2', data=b,compression=\"gzip\", compression_opts=9)\n",
    "d3=h5f.create_dataset('dataset_3', data=a,compression=\"lzf\")\n",
    "#h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in h5f:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(h5f.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(h5f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1.attrs['temperature']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(d1.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f.get('dataset_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( a, open( \"save.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "def guardarHDF5(nombreArchivo, valor, nuevo=False):\n",
    "    \"\"\"\n",
    "    Los dataset son para almacenar objetos grandes (ej: numpy.ndarray, listas)\n",
    "    Los atributos estan asociados a un dataset dado, y son mas eficientes \n",
    "    para objetos mas pequeÃ±os (escalares y strings)\n",
    "    \n",
    "    r. Read only, the file must exist.\n",
    "    r+. Read and write, the file must exist.\n",
    "    w. Create file, overwrite if it exists.\n",
    "    x. Create file, fail if it exists.\n",
    "    a. Read and write, create otherwise\n",
    "    \n",
    "    >>> guardarHDF5(\"coso.cupdyle\",{'nelo':10, 'nelo2':115}, True)\n",
    "    >>> guardarHDF5(\"coso.cupdyle\",{'nelo':np.asarray([[1,2,3],[4,5,6]]), 'nelo2':115}, False)\n",
    "    \n",
    "    \n",
    "    >>> h5f = h5py.File(\"coso.cupdyle\", 'r')\n",
    "    >>> list(h5f.keys())\n",
    "    >>> list(h5f['atributos'].attrs)\n",
    "    #h5f['nelo'].value\n",
    "    #h5f['nelo2'].value\n",
    "    #h5f['atributos'].attrs[\"nelo\"]\n",
    "\n",
    "    #h5f['atributos'].value\n",
    "    \n",
    "    #h5f['atributos'].attrs['nelo']\n",
    "    >>> h5f.close()\n",
    "    \"\"\"\n",
    "    if nuevo:\n",
    "        h5f = h5py.File(nombreArchivo, 'w')\n",
    "        h5f.create_dataset('atributos', data=0)\n",
    "    else:\n",
    "        h5f = h5py.File(nombreArchivo, 'r+')\n",
    "        for k in valor.keys():\n",
    "            assert k in h5f.keys() or k in h5f['atributos'].attrs\n",
    "\n",
    "    for k in valor.keys():\n",
    "        es_atributo=True\n",
    "        if isinstance(valor[k],np.ndarray):\n",
    "            if valor[k].shape[0]>1 or valor[k].shape[1]>1:\n",
    "                es_atributo=False\n",
    "        if isinstance(valor[k], list):\n",
    "            es_atributo=False\n",
    "            \n",
    "        if es_atributo:\n",
    "            h5f['atributos'].attrs[k] = valor[k]\n",
    "            try:\n",
    "                # si se guardo como atributo, trato de borrar si hay un dataset\n",
    "                h5f.__delitem__(k)\n",
    "            except:\n",
    "                pass\n",
    "                #h5f.close()\n",
    "        else:\n",
    "            # siempre crea uno nuevo, por lo tanto tener cuidado, no guarda listas\n",
    "            h5f.create_dataset(str(k), data=valor[k],compression=\"gzip\", compression_opts=9)\n",
    "            try:\n",
    "                # si se guardo como dataset, trato de borrar si hay un atributo\n",
    "                h5f['atributos'].attrs.__delitem__(k)\n",
    "            except:\n",
    "                pass\n",
    "                #h5f.close()\n",
    "    h5f.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guardarHDF5(\"coso.cupdyle\",{'nelo':10, 'nelo2':1}, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File(\"coso.cupdyle\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(h5f.keys())\n",
    "#h5f['nelo'].value\n",
    "h5f['nelo2'].value\n",
    "#h5f['atributos'].attrs[\"nelo\"]\n",
    "\n",
    "#h5f['atributos'].value\n",
    "#list(h5f['atributos'].attrs)\n",
    "#h5f['atributos'].attrs['nelo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat=np.asarray([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(dat) is np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guardarHDF5(\"coso.cupdyle\",{'nelo':dat, 'nelo2':115}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guardarHDF5(\"coso.cupdyle\",{'nelo':10, 'nelo2':115}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cargarHDF5(nombreArchivo, clave):\n",
    "    \"\"\"\n",
    "    carga la clave almacenada en el archivo del tipo HDF5.\n",
    "    si clave es None, carga todos los datos y los devuelve como un diccionario, cuidado con la dimension\n",
    "    \n",
    "    r. Read only, the file must exist.\n",
    "    r+. Read and write, the file must exist.\n",
    "    w. Create file, overwrite if it exists.\n",
    "    x. Create file, fail if it exists.\n",
    "    a. Read and write, create otherwise\n",
    "    \n",
    "    >>> print(cargarHDF5(\"coso.cupdyle\",\"nelo2\"))\n",
    "    ... 115\n",
    "    >>> print(cargarHDF5(\"coso.cupdyle\",\"nelo2\"))\n",
    "    ...    {'nelo2': 115, 'nelo': array([[1, 2, 3], [4, 5, 6]])}\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    datos = None\n",
    "    \n",
    "    h5f = h5py.File(nombreArchivo, 'r')\n",
    "    \n",
    "    if clave is not None:\n",
    "        if isinstance(clave, list):\n",
    "            datos = {}\n",
    "            for k in clave:\n",
    "                try:\n",
    "                    # busca primero si es un atributo\n",
    "                    datos[k] = h5f[\"atributos\"].attrs[k]\n",
    "                except:\n",
    "                    try:\n",
    "                        # busca si es un dataset\n",
    "                        datos[k] = np.asarray(h5f[k])\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            try:\n",
    "                # busca primero si es un atributo\n",
    "                datos = h5f[\"atributos\"].attrs[clave]\n",
    "            except:\n",
    "                try:\n",
    "                    # busca si es un dataset\n",
    "                    datos = np.asarray(h5f[clave])\n",
    "                except:\n",
    "                    pass\n",
    "    else: #clave es none, devuelvo todo, ojo\n",
    "        datos = {}\n",
    "        for k in h5f.keys():\n",
    "            if k == 'atributos':\n",
    "                for k in h5f['atributos'].attrs:\n",
    "                    datos[k] = h5f['atributos'].attrs[k]\n",
    "            else:\n",
    "                datos[k] = np.asarray(h5f[k])        \n",
    "        \n",
    "    h5f.close()\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cargarHDF5(\"coso.cupdyle\",\"nelo2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cargarHDF5(\"coso.cupdyle\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shelve\n",
    "def guardarSHELVE(nombreArchivo, valor, nuevo=False):\n",
    "    \"\"\"\n",
    "    alamcena todos los datos en archivos pickle individuales\n",
    "    se almacenan listas sin problemas y casi cualquier tipo de objeto\n",
    "    \n",
    "    >>> guardarSHELVE(\"cosoS.cupydle\", {'nelo':[12]}, nuevo=False)\n",
    "    \n",
    "    >>> shelf = shelve.open(\"cosoS.cupydle\", flag='r', writeback=False, protocol=2)\n",
    "    >>> shelf['nelo']\n",
    "    ... [11, 12]\n",
    "    >>> shelf.close()\n",
    "    \"\"\"\n",
    "    if nuevo:\n",
    "        shelf = shelve.open(nombreArchivo, flag='n', writeback=False, protocol=2)\n",
    "    else:\n",
    "        shelf = shelve.open(nombreArchivo, flag='w', writeback=False, protocol=2)\n",
    "        for k in valor.keys():\n",
    "            assert k in shelf.keys(), \"Clave no encontrada en el archivo\"\n",
    "            \n",
    "    for key in valor.keys():\n",
    "        if isinstance(valor[key] ,list):\n",
    "            if nuevo:\n",
    "                shelf[key] = valor[key]\n",
    "            else: # ya hay guardada una lista\n",
    "                tmp = shelf[key]\n",
    "                tmp.extend(valor[key])\n",
    "                shelf[key] = tmp\n",
    "                del tmp\n",
    "        else:\n",
    "            shelf[key] = valor[key]\n",
    "    shelf.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "guardarSHELVE(\"cosoS.cupydle\", {'nelo':[12], 'coso':numpy.asarray([[1,2],[2,3]])}, nuevo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shelf = shelve.open(\"cosoS.cupydle\", flag='r', writeback=False, protocol=2)\n",
    "shelf['nelo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cargarSHELVE(nombreArchivo, clave):\n",
    "    \"\"\"\n",
    "    carga la clave almacenada en el archivo del tipo shelve.\n",
    "    si clave es None, carga todos los datos y los devuelve como un diccionario, cuidado con la dimension\n",
    "    \n",
    "    'r' Open existing database for reading only (default)\n",
    "    'w' Open existing database for reading and writing\n",
    "    'c' Open database for reading and writing, creating it if it doesnât exist\n",
    "    'n' Always create a new, empty database, open for reading and writing\n",
    "    \n",
    "    >>> print(cargarHDF5(\"coso.cupdyle\",\"nelo2\"))\n",
    "    ... 115\n",
    "    >>> print(cargarHDF5(\"coso.cupdyle\",None))\n",
    "    ...    {'nelo2': 115, 'nelo': array([[1, 2, 3], [4, 5, 6]])}\n",
    "    >>> print(cargarHDF5(\"coso.cupdyle\",\"nelo\"))\n",
    "    ... [12]\n",
    "    \"\"\"\n",
    "    datos = None\n",
    "    with shelve.open(nombreArchivo, flag='r', writeback=False, protocol=2) as shelf:\n",
    "        if clave is not None:\n",
    "            assert clave in shelf.keys(), \"clave no almacenada \" + str(clave)\n",
    "            datos = shelf[clave]\n",
    "        else:\n",
    "            datos = {key: shelf[key] for key in shelf.keys()}\n",
    "        shelf.close()\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cargarSHELVE(\"cosoS.cupydle\", \"nelo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tipos de datos compuestos\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "# Create the compound datatype.\n",
    "dtype = np.dtype([(\"id\", np.int32), \n",
    "                  (\"data\",      h5py.special_dtype(vlen=str)),\n",
    "                      (\"Temperature\",   np.float),\n",
    "                      (\"Pressure\",      np.float)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.asarray([[1,2,3],[4,5,6]])\n",
    "b=np.asarray([[7,8,9],[10,11,12]])\n",
    "import h5py\n",
    "h5f = h5py.File(\"prueba.cupydle\", 'w')\n",
    "\n",
    "d1 = h5f.create_dataset('dato1', data = 10)\n",
    "\n",
    "g1 = h5f.create_group('pesos')\n",
    "g1.create_dataset('pesos1', data = a)\n",
    "g1.create_dataset('pesos2', data = b)\n",
    "\n",
    "d1 = h5f.create_dataset('dato2', data = 110)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File(\"prueba.cupydle\", 'r')\n",
    "print(list(h5f.keys()))\n",
    "print(h5f['dato1'].value)\n",
    "print(list(h5f['pesos'].keys()))\n",
    "print(type(h5f['pesos']))\n",
    "print(isinstance(h5f['dato1'], h5py.Group))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos = {'tipo':                'mlp',\n",
    "         'nombre':              \"coso\",\n",
    "         'numpy_rng':           \"cosa\",\n",
    "         'pesos':               [],\n",
    "         'pesos_iniciales':     [],\n",
    "         'bias':                [],\n",
    "         'bias_iniciales':      [],\n",
    "         'tasaAprendizaje':     0.0,\n",
    "         'regularizadorL1':     0.0,\n",
    "         'regularizadorL2':     0.0,\n",
    "         'momento':             0.0,\n",
    "         'epocas':              0,\n",
    "         'toleranciaError':     0.0,\n",
    "         'tiempoMaximo':        0,\n",
    "         'costoTRN':            0,\n",
    "         'costoVAL':            0,\n",
    "         'costoTST':            0,\n",
    "         'activacion':          'sigmoidea'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "def guardarHDF5(nombreArchivo, valor, nuevo=False):\n",
    "    \"\"\"\n",
    "    Los dataset son para almacenar objetos grandes (ej: numpy.ndarray, listas)\n",
    "    Los atributos estan asociados a un dataset dado, y son mas eficientes \n",
    "    para objetos mas pequeÃ±os (escalares y strings)\n",
    "    \n",
    "    r. Read only, the file must exist.\n",
    "    r+. Read and write, the file must exist.\n",
    "    w. Create file, overwrite if it exists.\n",
    "    x. Create file, fail if it exists.\n",
    "    a. Read and write, create otherwise\n",
    "    \n",
    "    >>> guardarHDF5(\"coso.cupdyle\",{'nelo':10, 'nelo2':115}, True)\n",
    "    >>> guardarHDF5(\"coso.cupdyle\",{'nelo':np.asarray([[1,2,3],[4,5,6]]), 'nelo2':115}, False)\n",
    "    \n",
    "    \n",
    "    >>> h5f = h5py.File(\"coso.cupdyle\", 'r')\n",
    "    >>> list(h5f.keys())\n",
    "    >>> list(h5f['atributos'].attrs)\n",
    "    #h5f['nelo'].value\n",
    "    #h5f['nelo2'].value\n",
    "    #h5f['atributos'].attrs[\"nelo\"]\n",
    "\n",
    "    #h5f['atributos'].value\n",
    "    \n",
    "    #h5f['atributos'].attrs['nelo']\n",
    "    >>> h5f.close()\n",
    "    \"\"\"\n",
    "    if nuevo:\n",
    "        h5f = h5py.File(nombreArchivo, 'w')\n",
    "        h5f.create_dataset('atributos', data=0)\n",
    "    else:\n",
    "        h5f = h5py.File(nombreArchivo, 'r+')\n",
    "        for k in valor.keys():\n",
    "            assert k in h5f.keys() or k in h5f['atributos'].attrs\n",
    "\n",
    "    for k in valor.keys():\n",
    "        es_atributo=True\n",
    "        if isinstance(valor[k],np.ndarray):\n",
    "            if valor[k].shape[0]>1 or valor[k].shape[1]>1:\n",
    "                es_atributo=False\n",
    "        if isinstance(valor[k], list):\n",
    "            es_atributo=False\n",
    "            \n",
    "        if es_atributo:\n",
    "            h5f['atributos'].attrs[k] = valor[k]\n",
    "            try:\n",
    "                # si se guardo como atributo, trato de borrar si hay un dataset\n",
    "                h5f.__delitem__(k)\n",
    "            except:\n",
    "                pass\n",
    "                #h5f.close()\n",
    "        else:\n",
    "            if nuevo:\n",
    "                if isinstance(valor[k], list):\n",
    "                    h5f.create_group(k)\n",
    "                else:\n",
    "                    h5f.create_dataset(str(k), data=valor[k],compression=\"gzip\", compression_opts=9)\n",
    "            else:\n",
    "                if isinstance(h5f[k], h5py.Group):\n",
    "                    #if isinstance(valor[k], list):\n",
    "                    # es como una lista, debo determinar el indice e incrementarlo dentro del grupo (lista)\n",
    "                    dd=h5f[k].items()\n",
    "                    import collections\n",
    "                    dd = collections.OrderedDict(sorted(dd))\n",
    "                    cantidad = len(dd)\n",
    "                    #print(valor[k])\n",
    "                    #print(k+str(cantidad))\n",
    "                    h5f[k].create_dataset(k+str(cantidad),data=valor[k][0])\n",
    "                else:\n",
    "                    h5f[k] = valor[k]\n",
    "            try:\n",
    "                # si se guardo como dataset, trato de borrar si hay un atributo\n",
    "                h5f['atributos'].attrs.__delitem__(k)\n",
    "            except:\n",
    "                pass\n",
    "                #h5f.close()\n",
    "    h5f.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "h5f = h5py.File(\"prueba.cupydle\", 'r')\n",
    "for k in h5f.keys():\n",
    "    print(k,h5f[k])\n",
    "print(list(h5f['pesos'].keys()))\n",
    "print(h5f[\"atributos\"].attrs)\n",
    "print(h5f['pesos']['pesos1'].value)\n",
    "print(h5f['pesos'])\n",
    "print([v.value for k,v in h5f['pesos'].items()])\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.asarray([[1,2,3],[4,5,6]])\n",
    "b=np.asarray([[7,8,9],[10,11,12]])\n",
    "import h5py\n",
    "h5f = h5py.File(\"prueba.cupydle\", 'w')\n",
    "for k in datos.keys():\n",
    "    if isinstance(datos[k], list):\n",
    "        h5f.create_group(k)\n",
    "        try:\n",
    "            h5f.create_group(k)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        h5f.create_dataset(k, data=datos[k])\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd={'pesos3':0, 'pesos2':1}\n",
    "import collections\n",
    "dd = collections.OrderedDict(sorted(dd.items()))\n",
    "for k, v in dd.items(): \n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardarHDF5(\"prueba.cupydle\", datos, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.asarray([[7,8,9],[10,11,12]])\n",
    "guardarHDF5(\"prueba.cupydle\",{'pesos':a+1}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File(\"prueba.cupydle\", 'r')\n",
    "import collections\n",
    "dd = h5f.items()\n",
    "dd = collections.OrderedDict(sorted(dd))\n",
    "print(dd)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cargarHDF5(nombreArchivo, clave):\n",
    "    \"\"\"\n",
    "    carga la clave almacenada en el archivo del tipo HDF5.\n",
    "    si clave es None, carga todos los datos y los devuelve como un diccionario, cuidado con la dimension\n",
    "    \n",
    "    r. Read only, the file must exist.\n",
    "    r+. Read and write, the file must exist.\n",
    "    w. Create file, overwrite if it exists.\n",
    "    x. Create file, fail if it exists.\n",
    "    a. Read and write, create otherwise\n",
    "    \n",
    "    >>> print(cargarHDF5(\"coso.cupdyle\",\"nelo2\"))\n",
    "    ... 115\n",
    "    >>> print(cargarHDF5(\"coso.cupdyle\",\"nelo2\"))\n",
    "    ...    {'nelo2': 115, 'nelo': array([[1, 2, 3], [4, 5, 6]])}\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    datos = None\n",
    "    \n",
    "    h5f = h5py.File(nombreArchivo, 'r')\n",
    "    \n",
    "    if clave is not None:\n",
    "        if isinstance(clave, list):\n",
    "            datos = {}\n",
    "            for k in clave:\n",
    "                # busca primero si es un atributo\n",
    "                if k in h5f[\"atributos\"].attrs:\n",
    "                    datos[k] = h5f[\"atributos\"].attrs[k]\n",
    "                elif isinstance(h5f[k], h5py.Group):    # busca si es un grupo (lista)\n",
    "                    #datos[k] = []\n",
    "                    #datos[k] = np.asarray(h5f[k])\n",
    "                    # guarda una lista con los arrays contenidos en el grupo, ordenado\n",
    "                    import collections\n",
    "                    datos[k] = [np.asarray(val.value) for key, val in collections.OrderedDict(sorted(h5f[k].items())).items()]\n",
    "                else:\n",
    "                    datos[k] = np.asarray(h5f[k])                                             \n",
    "        else:\n",
    "            # busca primero si es un atributo\n",
    "            if clave in h5f[\"atributos\"].attrs:\n",
    "                datos = h5f[\"atributos\"].attrs[clave]\n",
    "            elif isinstance(h5f[clave], h5py.Group):    # busca si es un grupo (lista)\n",
    "                # guarda una lista con los arrays contenidos en el grupo, ordenado\n",
    "                import collections\n",
    "                datos = [np.asarray(val.value) for key, val in collections.OrderedDict(sorted(h5f[clave].items())).items()]\n",
    "            else:\n",
    "                datos = np.asarray(h5f[clave])\n",
    "    else: #clave es none, devuelvo todo, ojo\n",
    "        datos = {}\n",
    "        for k in h5f.keys():\n",
    "            if k == 'atributos':\n",
    "                for k in h5f['atributos'].attrs:\n",
    "                    datos[k] = h5f['atributos'].attrs[k]\n",
    "            elif isinstance(h5f[k], h5py.Group):\n",
    "                import collections\n",
    "                datos[k] = [np.asarray(val.value) for key, val in collections.OrderedDict(sorted(h5f[k].items())).items()]\n",
    "            else:\n",
    "                datos[k] = np.asarray(h5f[k])        \n",
    "    h5f.close()\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([7, 8, 9]), array([7, 8, 9]), array([ 8,  9, 10])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cargarHDF5(\"prueba.cupydle\",['nombre', 'pesos'])\n",
    "cargarHDF5(\"prueba.cupydle\",None)\n",
    "cargarHDF5(\"prueba.cupydle\",'pesos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardarHDF5(\"prueba.cupydle\", datos, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 7,  8,  9],\n",
      "       [10, 11, 12]])]\n",
      "pesos2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.asarray([[7,8,9],[10,11,12]])\n",
    "guardarHDF5(\"prueba.cupydle\",{'pesos':[a]}, False)\n",
    "guardarHDF5(\"prueba.cupydle\",{'tipo':\"mlp2\"}, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlp2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cargarHDF5(\"prueba.cupydle\",\"tipo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos = {'nombre': 'mlp', 'valor': 0.0, 'pesos': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nombre': 'mlp', 'pesos': [], 'valor': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
